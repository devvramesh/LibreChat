# Do not edit this file directly. Use a ‘docker-compose.override.yaml’ file if you can.
# Refer to `docker-compose.override.yaml.example’ for some sample configurations.

services:
  api:
    container_name: LibreChat
    ports:
      - "${PORT}:${PORT}"
    depends_on:
      - mongodb
      - rag_api
    image: ghcr.io/danny-avila/librechat-dev:latest
    restart: always
    user: "${UID}:${GID}"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - HOST=0.0.0.0
      - MONGO_URI=mongodb://mongodb:27017/LibreChat
      - MEILI_HOST=http://meilisearch:7700
      - RAG_PORT=${RAG_PORT:-8000}
      - RAG_API_URL=http://rag_api:${RAG_PORT:-8000}
    volumes:
    - type: bind
      source: ./.env
      target: /app/.env

    - type: bind
      source: ./librechat.yaml
      target: /app/librechat.yaml

    - ./images:/app/client/public/images
    - ./uploads:/app/uploads
    - ./logs:/app/logs
    - ./mcp:/app/mcp
  mongodb:
    container_name: chat-mongodb
    image: mongo:8.0.17
    restart: always
    user: "${UID}:${GID}"
    volumes:
      - ./data-node:/data/db
    command: mongod --noauth
  meilisearch:
    container_name: chat-meilisearch
    image: getmeili/meilisearch:v1.12.3
    restart: always
    user: "${UID}:${GID}"
    environment:
      - MEILI_HOST=http://meilisearch:7700
      - MEILI_NO_ANALYTICS=true
      - MEILI_MASTER_KEY=${MEILI_MASTER_KEY}
    volumes:
      - ./meili_data_v1.12:/meili_data
    ports:
      - "7700:7700"
  vectordb:
    container_name: vectordb
    image: pgvector/pgvector:0.8.0-pg15-trixie
    environment:
      POSTGRES_DB: mydatabase
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: mypassword
    restart: always
    volumes:
      - pgdata2:/var/lib/postgresql/data
  rag_api:
    container_name: rag_api
    image: ghcr.io/danny-avila/librechat-rag-api-dev-lite:latest
    environment:
      - DB_HOST=vectordb
      - RAG_PORT=${RAG_PORT:-8000}
    restart: always
    depends_on:
      - vectordb
    env_file:
      - .env
  mcp_chat_search:
    image: node:22-alpine
    working_dir: /app
    volumes:
      - ./mcp/chat-search:/app
    command: ["sh", "-c", "npm install && node index.js"]
    environment:
      MEILI_URL: "http://chat-meilisearch:7700"
      MEILI_MASTER_KEY: "${MEILI_MASTER_KEY}"
    depends_on:
      - meilisearch
  searxng:
    container_name: searxng
    image: searxng/searxng:latest
    restart: always
    volumes:
      - ./searxng:/etc/searxng
    ports:
      - "8080:8080"
  mcp_web_search:
    image: node:22-alpine
    working_dir: /app
    volumes:
      - ./mcp/web-search:/app
    command: ["sh", "-c", "npm install && node index.js"]
    environment:
      SEARXNG_URL: "http://searxng:8080"
    depends_on:
      - searxng
  # Ollama - local LLM for memory extraction
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    # Uncomment below if you have NVIDIA GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    restart: unless-stopped
  # Qdrant - vector database for semantic search
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    volumes:
      - qdrant_data:/qdrant/storage
    ports:
      - "6333:6333"
    restart: unless-stopped
  # MCP Memory Server with Mem0
  # Pricing: Anthropic Haiku ~$0.0004 per memory operation (1000 ops ≈ $0.40)
  mcp_memory:
    build:
      context: ./mcp/memory
      dockerfile: Dockerfile
    container_name: mcp_memory
    environment:
      # LLM Provider: "anthropic" (fast), "bedrock" (future), "ollama" (slow fallback)
      LLM_PROVIDER: "anthropic"
      
      # Anthropic (recommended - fastest, ~0.25s per operation)
      ANTHROPIC_API_KEY: "${ANTHROPIC_API_KEY}"
      ANTHROPIC_MODEL: "claude-3-haiku-20240307"
      
      # AWS Bedrock (uncomment to use instead of Anthropic)
      # LLM_PROVIDER: "bedrock"
      # AWS_REGION: "${AWS_REGION:-us-east-1}"
      # BEDROCK_MODEL: "anthropic.claude-3-haiku-20240307-v1:0"
      
      # Ollama for embeddings (always used - fast and free)
      OLLAMA_HOST: "http://ollama:11434"
      OLLAMA_EMBED_MODEL: "nomic-embed-text"
      # Ollama LLM fallback (uncomment if LLM_PROVIDER=ollama)
      OLLAMA_LLM_MODEL: "llama3.1:8b"
      
      # Qdrant vector store
      QDRANT_HOST: "qdrant"
      QDRANT_PORT: "6333"
      
      PORT: "3003"
    ports:
      - "3003:3003"
    depends_on:
      - ollama
      - qdrant
    restart: unless-stopped


volumes:
  pgdata2:
  ollama_data:
  qdrant_data:
